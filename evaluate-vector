#!/usr/bin/env python
import argparse # optparse is deprecated
import VectorMap.py as vm
from itertools import islice # slicing for iterators
 
 # unneeded, replace with cosign similarity calculations
def word_matches(h, ref):
    return sum(1 for w in h if w in ref)

def preprocess(line):
    line = line.lower()
    exclude = set(string.punctuation)
    line = ''.join(ch if ch not in exclude else ' ' for ch in line)
    return line
 
def main():
    parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    parser.add_argument('-i', '--input', default='data/hyp1-hyp2-ref',
            help='input file (default data/hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
    opts = parser.parse_args()
 
    # we create a generator and avoid loading all sentences into a list
    def sentences():
        with open(opts.input) as f:
            for pair in f:
                yield [preprocess(sentence) for sentence in pair.split(' ||| ')]
 
    # note: the -n option does not work in the original code
    for h1, h2, ref in islice(sentences(), opts.num_sentences):
        v_ref = vm.sentence_to_vector(ref)
        v_h1 = vm.sentence_to_vector(h1)
        v_h2 = vm.sentence_to_vector(h2)
        h1_match = sentence_similarity(v_h1, v_ref)
        h2_match = sentence_similarity(v_h2, v_ref)
        # 1 if distance between h1 and ref is less than h2 and ref
        print(1 if h1_match > h2_match else # \begin{cases}
                (0 if h1_match == h2_match
                    else -1)) # \end{cases}
 
# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
